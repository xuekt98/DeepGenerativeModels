runner: "GANModelRunner" # runner to be used
training: # training options
  n_epochs: 200              # maximum training epochs (integer > 0)
  n_steps: 50000            # maximum training steps (integer > 0)
  save_interval_epoch: 5     # save checkpoint epoch interval (integer > 0)
  # save_interval_step: 1      # save checkpoint step interval (integer > 0)
  val_interval_epoch: 5      # validation epoch interval (integer > 0)
  val_interval_step: 1000     # validation step interval (integer > 0)
  sample_interval: 2       # sample interval epoch length (number in (0, +inf))
  accumulate_grad_batches: 1 # accumulate grad batches (integer > 0)

testing: # testing options
  sample_num: 2000 # sample image number (integer > 0)

data: # dataset config
  dataset_type: 'official' # options: {'official', 'custom_aligned', 'custom_single', 'custom_inpainting'} (string)
  dataset_config:
    dataset_name: 'MNIST' # dataset name, part of result path (string)
    dataset_path: '/media/x/disk/datasets/MNIST' # dataset path (string)
    image_size: 28     # image size (integer > 0)
    channels: 1        # image channels (integer > 0)
    to_normal: False    # normalize to [-1, 1] (bool)
    flip: True         # random flip image to augment dataset (bool)
    resize: True       # resize image to train and test (bool)
    random_crop: False # random crop image to augment dataset (bool)
    crop_p1: 0.5       # random crop probability 1, only used when ramdom crop is True (float in [0, 1])
    crop_p2: 1.        # random crop probability 2, only used when ramdom crop is True (float in [crop_p1, 1])
  num_workers: 8       # dataloader worker numbers (integer > 0)
  train_batch_size: 128  # train batch size (integer > 0)
  val_batch_size: 4    # validation batch size (integer > 0)
  test_batch_size: 4   # test batch size (integer > 0)

model: # model config
  model_name: 'GANModel' # model name, part of result path (string)

  generator:
    num_linear_layers: 3
    ch_mult: !!python/tuple
      - 1
      - 2
      - 4
      - 2
      - 1
    in_channels: 256 # 28x28x1
    out_channels: 784
    mid_channels: 1024
    dropout: 0.
    activation: 'Sigmoid' # options {'ReLU', 'Sigmoid', 'LeakyReLU'}
    normalization: 'batch_norm' # options {'batch_norm', 'group_norm'}

  discriminator:
    num_linear_layers: 2
    ch_mult: !!python/tuple
      - 4
      - 2
      - 1
    in_channels: 784 # 28x28x1
    out_channels: 1
    mid_channels: 256
    dropout: 0.
    activation: 'ReLU' # options {'ReLU', 'Sigmoid', 'LeakyReLU'}
    normalization: 'batch_norm' # options {'batch_norm', 'group_norm'}

  optimizer_g:
    optimizer: 'Adam'
    weight_decay: 0.000
    lr: 1.e-4
    beta1: 0.9

  optimizer_d:
    optimizer: 'Adam'
    weight_decay: 0.000
    lr: 1.e-6
    beta1: 0.9

